{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":457,"status":"ok","timestamp":1709973403352,"user":{"displayName":"Abhay Pancholi","userId":"05758217017184540702"},"user_tz":-330},"id":"4961e54c","outputId":"98703808-5d30-4c9e-c543-4a942936ca11"},"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.10.12\n"]}],"source":["!python --version"],"id":"4961e54c"},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7447,"status":"ok","timestamp":1709973411401,"user":{"displayName":"Abhay Pancholi","userId":"05758217017184540702"},"user_tz":-330},"id":"wfML3N3UyK4S","outputId":"be30b36b-ed93-4db2-e3af-7325f7426883"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.1.24-py3-none-any.whl (719 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.5/719.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.0+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.16.0+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n","Collecting thop>=0.1.1 (from ultralytics)\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.49.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n","Installing collected packages: thop, ultralytics\n","Successfully installed thop-0.1.1.post2209072238 ultralytics-8.1.24\n"]}],"source":["!pip install ultralytics"],"id":"wfML3N3UyK4S"},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":5833,"status":"ok","timestamp":1709973417229,"user":{"displayName":"Abhay Pancholi","userId":"05758217017184540702"},"user_tz":-330},"id":"5ea3ead0","outputId":"d14e24c2-64d3-46bd-9c13-98fb1a9cf91b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'8.1.24'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["import ultralytics\n","ultralytics.__version__"],"id":"5ea3ead0"},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5670,"status":"ok","timestamp":1709973422885,"user":{"displayName":"Abhay Pancholi","userId":"05758217017184540702"},"user_tz":-330},"id":"7Q7wFzH-yUpU","outputId":"acadc49d-9847-43df-9e20-c6615407092d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}],"source":["!pip install torch"],"id":"7Q7wFzH-yUpU"},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1709973422886,"user":{"displayName":"Abhay Pancholi","userId":"05758217017184540702"},"user_tz":-330},"id":"6801a5a9","outputId":"59d8a79b-6587-498c-ac4d-01da61440969"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.1.0+cu121'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}],"source":["import torch\n","torch.__version__"],"id":"6801a5a9"},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1709973422886,"user":{"displayName":"Abhay Pancholi","userId":"05758217017184540702"},"user_tz":-330},"id":"9627ba4a","outputId":"b3f14a84-2eee-4aaa-f090-6a0ba213de55"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Tesla T4'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}],"source":["torch.cuda.get_device_name(0)"],"id":"9627ba4a"},{"cell_type":"markdown","metadata":{"id":"30beac26"},"source":["# Detect, track and count Persons"],"id":"30beac26"},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32104,"status":"ok","timestamp":1709973454973,"user":{"displayName":"Abhay Pancholi","userId":"05758217017184540702"},"user_tz":-330},"id":"lJhf8ip6yj-Y","outputId":"14feca7d-0b07-423b-aa67-7d31939e81d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"lJhf8ip6yj-Y"},{"cell_type":"code","execution_count":8,"metadata":{"id":"c23349aa","executionInfo":{"status":"ok","timestamp":1709973454973,"user_tz":-330,"elapsed":14,"user":{"displayName":"Abhay Pancholi","userId":"05758217017184540702"}}},"outputs":[],"source":["# %cd yolov8_DeepSORT"],"id":"c23349aa"},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6542,"status":"ok","timestamp":1709973461505,"user":{"displayName":"Abhay Pancholi","userId":"05758217017184540702"},"user_tz":-330},"id":"7ac57944","outputId":"27eee42b-c0a9-4bb6-9b57-afc7461659cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n.pt to 'yolov8n.pt'...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 6.23M/6.23M [00:00<00:00, 114MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","image 1/1 /content/drive/MyDrive/Major Project/Images/Jumping/IMG_20231027_180529681_BURST0011.jpg: 640x480 1 person, 5 potted plants, 84.6ms\n","Speed: 11.8ms preprocess, 84.6ms inference, 993.3ms postprocess per image at shape (1, 3, 640, 480)\n","Results saved to \u001b[1mruns/detect/predict\u001b[0m\n","[58.0, 0.0, 58.0, 58.0, 58.0, 58.0]\n","Class: potted plant\n","Class: person\n","Class: potted plant\n","Class: potted plant\n","Class: potted plant\n","Class: potted plant\n"]}],"source":["from ultralytics import YOLO\n","\n","import time\n","import torch\n","import cv2\n","import torch.backends.cudnn as cudnn\n","from PIL import Image\n","import colorsys\n","import numpy as np"],"id":"7ac57944"},{"cell_type":"markdown","metadata":{"id":"461c7b6e"},"source":["# DeepSORT"],"id":"461c7b6e"},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1709973461506,"user":{"displayName":"Abhay Pancholi","userId":"05758217017184540702"},"user_tz":-330},"id":"2nmBkwywz0Jg","outputId":"b22f9044-ff6e-4e25-f815-04ef3d2e7a74"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Tracking-and-counting-Using-YOLOv8-and-DeepSORT\n"]}],"source":["%cd /content/drive/MyDrive/Tracking-and-counting-Using-YOLOv8-and-DeepSORT/"],"id":"2nmBkwywz0Jg"},{"cell_type":"code","execution_count":11,"metadata":{"id":"945f584b","executionInfo":{"status":"ok","timestamp":1709973474382,"user_tz":-330,"elapsed":11496,"user":{"displayName":"Abhay Pancholi","userId":"05758217017184540702"}}},"outputs":[],"source":["import cv2\n","from deep_sort.utils.parser import get_config\n","from deep_sort.deep_sort import DeepSort\n","from deep_sort.sort.tracker import Tracker\n","\n","deep_sort_weights = 'deep_sort/deep/checkpoint/ckpt.t7'\n","tracker = DeepSort(model_path=deep_sort_weights, max_age=120)"],"id":"945f584b"},{"cell_type":"code","execution_count":29,"metadata":{"id":"2d74f1e2","executionInfo":{"status":"ok","timestamp":1709974482657,"user_tz":-330,"elapsed":449,"user":{"displayName":"Abhay Pancholi","userId":"05758217017184540702"}}},"outputs":[],"source":["# Define the video path\n","video_path = '/content/drive/MyDrive/Tracking-and-counting-Using-YOLOv8-and-DeepSORT/input_video.mp4'\n","\n","cap = cv2.VideoCapture(video_path)\n","\n","# Get the video properties\n","frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","fps = cap.get(cv2.CAP_PROP_FPS)\n","\n","# Define the codec and create VideoWriter object\n","fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","output_path = '/content/drive/MyDrive/Tracking-and-counting-Using-YOLOv8-and-DeepSORT/testnewopyolo.mp4'\n","out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"id":"2d74f1e2"},{"cell_type":"code","execution_count":30,"metadata":{"id":"09056afd","executionInfo":{"status":"ok","timestamp":1709974484766,"user_tz":-330,"elapsed":2,"user":{"displayName":"Abhay Pancholi","userId":"05758217017184540702"}}},"outputs":[],"source":["frames = []\n","\n","unique_track_ids = set()"],"id":"09056afd"},{"cell_type":"markdown","source":["# Main Code"],"metadata":{"id":"0_Vi2FY-fdD3"},"id":"0_Vi2FY-fdD3"},{"cell_type":"code","execution_count":31,"metadata":{"id":"-cJvslT71m1a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709974520270,"user_tz":-330,"elapsed":33485,"user":{"displayName":"Abhay Pancholi","userId":"05758217017184540702"}},"outputId":"9b96ca52-3bc0-4714-d3df-55f66ab07676"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","0: 480x640 1 person, 11.4ms\n","Speed: 1.7ms preprocess, 11.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.2ms\n","Speed: 1.3ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.2ms\n","Speed: 1.3ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.7ms\n","Speed: 1.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.0ms\n","Speed: 1.3ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.4ms\n","Speed: 2.5ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.0ms\n","Speed: 1.3ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.8ms\n","Speed: 1.6ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.1ms\n","Speed: 1.3ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 8.0ms\n","Speed: 1.3ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.4ms\n","Speed: 1.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.1ms\n","Speed: 1.2ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.1ms\n","Speed: 1.2ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.1ms\n","Speed: 1.4ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 10.1ms\n","Speed: 1.3ms preprocess, 10.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 8.0ms\n","Speed: 1.5ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 11.6ms\n","Speed: 1.6ms preprocess, 11.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 11.7ms\n","Speed: 1.4ms preprocess, 11.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 9.1ms\n","Speed: 1.4ms preprocess, 9.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 8.5ms\n","Speed: 1.4ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 12.3ms\n","Speed: 1.7ms preprocess, 12.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 8.9ms\n","Speed: 1.5ms preprocess, 8.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 10.5ms\n","Speed: 1.5ms preprocess, 10.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 13.2ms\n","Speed: 1.5ms preprocess, 13.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 12.6ms\n","Speed: 1.6ms preprocess, 12.6ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 10.1ms\n","Speed: 1.6ms preprocess, 10.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.1ms\n","Speed: 1.3ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.8ms\n","Speed: 1.3ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.2ms\n","Speed: 1.3ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 11.8ms\n","Speed: 1.6ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 6.9ms\n","Speed: 1.2ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 2 persons, 8.7ms\n","Speed: 1.3ms preprocess, 8.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 12.7ms\n","Speed: 1.7ms preprocess, 12.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 1 person, 12.4ms\n","Speed: 1.6ms preprocess, 12.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 2 persons, 8.7ms\n","Speed: 1.4ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 8.4ms\n","Speed: 1.4ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 1 person, 13.8ms\n","Speed: 1.5ms preprocess, 13.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 2 persons, 8.7ms\n","Speed: 1.5ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 9.3ms\n","Speed: 1.4ms preprocess, 9.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 8.5ms\n","Speed: 1.4ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 9.6ms\n","Speed: 1.4ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 1 person, 12.2ms\n","Speed: 1.6ms preprocess, 12.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 6.9ms\n","Speed: 1.3ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 11.5ms\n","Speed: 1.2ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.0ms\n","Speed: 1.3ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 10.0ms\n","Speed: 1.3ms preprocess, 10.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 6.9ms\n","Speed: 1.2ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.1ms\n","Speed: 1.2ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.0ms\n","Speed: 1.3ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.0ms\n","Speed: 1.2ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 11.4ms\n","Speed: 1.2ms preprocess, 11.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.2ms\n","Speed: 1.3ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.2ms\n","Speed: 1.2ms preprocess, 7.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 2 persons, 7.1ms\n","Speed: 1.3ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 7.5ms\n","Speed: 1.3ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 1 person, 7.9ms\n","Speed: 1.3ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.2ms\n","Speed: 1.4ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.2ms\n","Speed: 1.3ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 2 persons, 7.0ms\n","Speed: 1.2ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 7.4ms\n","Speed: 1.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 7.0ms\n","Speed: 1.3ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 1 person, 7.3ms\n","Speed: 1.3ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 2 persons, 7.3ms\n","Speed: 1.3ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 7.2ms\n","Speed: 1.3ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 7.1ms\n","Speed: 1.3ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 7.1ms\n","Speed: 1.3ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 7.0ms\n","Speed: 1.3ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 7.2ms\n","Speed: 1.3ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 1 person, 9.9ms\n","Speed: 1.4ms preprocess, 9.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.5ms\n","Speed: 1.3ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.8ms\n","Speed: 1.4ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.0ms\n","Speed: 1.2ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.7ms\n","Speed: 1.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.1ms\n","Speed: 1.2ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 6.9ms\n","Speed: 1.2ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.3ms\n","Speed: 1.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.1ms\n","Speed: 1.2ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.3ms\n","Speed: 1.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.3ms\n","Speed: 1.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.8ms\n","Speed: 1.2ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.4ms\n","Speed: 1.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.1ms\n","Speed: 1.3ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 6.9ms\n","Speed: 1.2ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.4ms\n","Speed: 1.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 2 persons, 7.8ms\n","Speed: 1.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 3 persons, 7.6ms\n","Speed: 1.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0, 0.0]\n","Class: person\n","Class: person\n","Class: person\n","\n","0: 480x640 3 persons, 7.8ms\n","Speed: 1.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0, 0.0]\n","Class: person\n","Class: person\n","Class: person\n","\n","0: 480x640 3 persons, 7.4ms\n","Speed: 1.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0, 0.0]\n","Class: person\n","Class: person\n","Class: person\n","\n","0: 480x640 3 persons, 9.0ms\n","Speed: 1.3ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0, 0.0]\n","Class: person\n","Class: person\n","Class: person\n","\n","0: 480x640 3 persons, 7.6ms\n","Speed: 1.3ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0, 0.0]\n","Class: person\n","Class: person\n","Class: person\n","\n","0: 480x640 3 persons, 9.6ms\n","Speed: 1.3ms preprocess, 9.6ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0, 0.0]\n","Class: person\n","Class: person\n","Class: person\n","\n","0: 480x640 3 persons, 8.7ms\n","Speed: 1.4ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0, 0.0]\n","Class: person\n","Class: person\n","Class: person\n","\n","0: 480x640 3 persons, 11.0ms\n","Speed: 1.4ms preprocess, 11.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0, 0.0]\n","Class: person\n","Class: person\n","Class: person\n","\n","0: 480x640 3 persons, 11.7ms\n","Speed: 1.8ms preprocess, 11.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0, 0.0]\n","Class: person\n","Class: person\n","Class: person\n","\n","0: 480x640 3 persons, 9.2ms\n","Speed: 1.5ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0, 0.0]\n","Class: person\n","Class: person\n","Class: person\n","\n","0: 480x640 3 persons, 9.6ms\n","Speed: 1.5ms preprocess, 9.6ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0, 0.0]\n","Class: person\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 8.4ms\n","Speed: 1.4ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 3 persons, 12.3ms\n","Speed: 1.6ms preprocess, 12.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0, 0.0]\n","Class: person\n","Class: person\n","Class: person\n","\n","0: 480x640 3 persons, 8.8ms\n","Speed: 1.5ms preprocess, 8.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0, 0.0]\n","Class: person\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 9.8ms\n","Speed: 1.3ms preprocess, 9.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 3 persons, 7.4ms\n","Speed: 1.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0, 0.0]\n","Class: person\n","Class: person\n","Class: person\n","\n","0: 480x640 1 person, 7.4ms\n","Speed: 1.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 2 persons, 7.2ms\n","Speed: 1.3ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 7.3ms\n","Speed: 1.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 7.4ms\n","Speed: 1.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 7.2ms\n","Speed: 1.2ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 7.1ms\n","Speed: 1.2ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 7.4ms\n","Speed: 1.2ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 8.1ms\n","Speed: 1.3ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 1 person, 7.6ms\n","Speed: 1.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 2 persons, 7.5ms\n","Speed: 1.4ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 1 person, 13.4ms\n","Speed: 1.5ms preprocess, 13.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 3 persons, 7.5ms\n","Speed: 1.3ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0, 0.0]\n","Class: person\n","Class: person\n","Class: person\n","\n","0: 480x640 3 persons, 7.0ms\n","Speed: 1.2ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0, 0.0]\n","Class: person\n","Class: person\n","Class: person\n","\n","0: 480x640 3 persons, 9.2ms\n","Speed: 1.3ms preprocess, 9.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0, 0.0]\n","Class: person\n","Class: person\n","Class: person\n","\n","0: 480x640 3 persons, 9.9ms\n","Speed: 1.3ms preprocess, 9.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0, 0.0]\n","Class: person\n","Class: person\n","Class: person\n","\n","0: 480x640 3 persons, 9.2ms\n","Speed: 1.4ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0, 0.0]\n","Class: person\n","Class: person\n","Class: person\n","\n","0: 480x640 3 persons, 13.0ms\n","Speed: 1.6ms preprocess, 13.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0, 0.0]\n","Class: person\n","Class: person\n","Class: person\n","\n","0: 480x640 3 persons, 8.8ms\n","Speed: 1.7ms preprocess, 8.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0, 0.0]\n","Class: person\n","Class: person\n","Class: person\n","\n","0: 480x640 3 persons, 9.3ms\n","Speed: 1.7ms preprocess, 9.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0, 0.0]\n","Class: person\n","Class: person\n","Class: person\n","\n","0: 480x640 3 persons, 8.5ms\n","Speed: 1.4ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0, 0.0]\n","Class: person\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 12.0ms\n","Speed: 1.5ms preprocess, 12.0ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 1 person, 10.7ms\n","Speed: 1.4ms preprocess, 10.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 2 persons, 12.1ms\n","Speed: 1.5ms preprocess, 12.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 7.0ms\n","Speed: 1.2ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 7.8ms\n","Speed: 1.3ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 8.8ms\n","Speed: 1.4ms preprocess, 8.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 11.0ms\n","Speed: 1.7ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 7.5ms\n","Speed: 1.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 10.3ms\n","Speed: 1.3ms preprocess, 10.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 7.3ms\n","Speed: 1.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 1 person, 7.6ms\n","Speed: 1.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.4ms\n","Speed: 1.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 1 person, 7.1ms\n","Speed: 1.3ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0]\n","Class: person\n","\n","0: 480x640 2 persons, 7.4ms\n","Speed: 1.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 7.5ms\n","Speed: 1.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n","\n","0: 480x640 2 persons, 7.2ms\n","Speed: 1.3ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","Cls value: [0.0, 0.0]\n","Class: person\n","Class: person\n"]}],"source":["# Placeholder function for extracting appearance features\n","def extract_appearance_features(frame, bounding_box):\n","    # Ensure the bounding box is valid before extracting features\n","    if bounding_box.size > 0:  # Check if the bounding box is not empty\n","        # Example: Extract color histogram as appearance feature\n","        # Convert the bounding box region to HSV color space\n","        hsv = cv2.cvtColor(bounding_box, cv2.COLOR_BGR2HSV)\n","\n","        # Calculate the color histogram\n","        hist_hue = cv2.calcHist([hsv], [0], None, [180], [0, 180])\n","        hist_saturation = cv2.calcHist([hsv], [1], None, [256], [0, 256])\n","        hist_value = cv2.calcHist([hsv], [2], None, [256], [0, 256])\n","\n","        # Normalize the histograms\n","        cv2.normalize(hist_hue, hist_hue, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)\n","        cv2.normalize(hist_saturation, hist_saturation, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)\n","        cv2.normalize(hist_value, hist_value, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)\n","\n","        # Concatenate the histograms as the appearance feature\n","        appearance_feature = np.concatenate((hist_hue.flatten(), hist_saturation.flatten(), hist_value.flatten()))\n","\n","        return appearance_feature\n","    else:\n","        return None  # Return None if the bounding box is empty or invalid\n","\n","# Placeholder function for matching appearances\n","def match_appearances(current_appearance, past_appearance):\n","    # Implement appearance matching logic here\n","\n","    # Iterate through appearance features in the current and past appearances\n","    for feature_id, current_feature in current_appearance.items():\n","        # Check if the feature_id exists in the past appearance dictionary\n","        if feature_id in past_appearance:\n","            # Calculate the Euclidean distance between current and past appearance features\n","            distance = np.linalg.norm(current_feature - past_appearance[feature_id])\n","\n","            # Set a threshold for similarity/dissimilarity\n","            threshold = 0.5  # Adjust this threshold based on your feature representation\n","\n","            # If the distance is below the threshold, consider it a match\n","            if distance < threshold:\n","                return feature_id  # Return the matched feature ID\n","\n","    # If no match is found, return None\n","    return None\n","# Initialize a dictionary to store appearance information for past tracks\n","i = 0\n","counter, fps, elapsed = 0, 0, 0\n","start_time = time.perf_counter()\n","appearance_memory = {}\n","\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","\n","    if ret:\n","        og_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","        frame = og_frame.copy()\n","\n","        model = YOLO(\"/content/yolov8n.pt\")  # load a pretrained model (recommended for training)\n","\n","        results = model(frame, device=0, classes=0, conf=0.8)\n","\n","        class_names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n","\n","        for result in results:\n","            boxes = result.boxes  # Boxes object for bbox outputs\n","            probs = result.probs  # Class probabilities for classification outputs\n","            cls = boxes.cls.tolist()  # Convert tensor to list\n","            xyxy = boxes.xyxy\n","            conf = boxes.conf\n","            xywh = boxes.xywh  # box with xywh format, (N, 4)\n","            print(\"Cls value:\", cls)\n","            for class_index in cls:\n","                class_name = class_names[int(class_index)]\n","                print(\"Class:\", class_name)\n","\n","        pred_cls = np.array(cls)\n","        conf = conf.detach().cpu().numpy()\n","        xyxy = xyxy.detach().cpu().numpy()\n","        bboxes_xywh = xywh\n","        bboxes_xywh = xywh.cpu().numpy()\n","        bboxes_xywh = np.array(bboxes_xywh, dtype=float)\n","\n","        tracks = tracker.update(bboxes_xywh, conf, og_frame)\n","\n","        # Initialize a dictionary to store current detections' appearances\n","        current_appearances = {}\n","\n","        for track in tracker.tracker.tracks:\n","            track_id = track.track_id\n","            # Get bounding box information\n","            x1, y1, x2, y2 = track.to_tlbr()\n","            bounding_box = frame[int(y1):int(y2), int(x1):int(x2)]\n","            # Extract appearance features for the current track\n","            appearance = extract_appearance_features(frame, bounding_box)\n","\n","            # Check if track ID exists in appearance memory\n","            if track_id in appearance_memory:\n","                # Compare current appearance with past appearances\n","                matched_id = match_appearances(current_appearances, appearance_memory[track_id])\n","\n","                if matched_id is not None:\n","                    # Update the track ID if matched with past appearance\n","                    track.track_id = matched_id\n","\n","            # Update the appearance memory with the current appearance\n","            appearance_memory[track_id] = appearance\n","\n","            # Draw bounding box and label with updated track ID\n","            for track in tracker.tracker.tracks:\n","              track_id = track.track_id\n","              x1, y1, x2, y2 = track.to_tlbr()\n","\n","              # Set color values for different tracks\n","              color = (255, 0, 0)  # Default color\n","\n","              # Update color based on track ID\n","              if track_id in appearance_memory:\n","                  color = (0, 255, 0)  # Set a different color if the appearance is matched\n","\n","              # Draw bounding box\n","              cv2.rectangle(og_frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n","\n","              # Create label with updated track ID\n","              label = f\"Track ID: {track_id}\"\n","\n","              # Display the label near the top-left corner of the bounding box\n","              cv2.putText(og_frame, label, (int(x1), int(y1 - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n","        # Other parts of the code: Counting, FPS calculation, writing frames, etc.\n","        person_count = len(tracks)\n","\n","        # Update FPS and display on frame (if needed)\n","        # Assuming fps is calculated elsewhere, display FPS on the top-left corner of the frame\n","        cv2.putText(og_frame, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n","\n","        # Writing the frame to the output video file\n","        out.write(cv2.cvtColor(og_frame, cv2.COLOR_RGB2BGR))\n","\n","    else:\n","        break\n","cap.release()\n","out.release()\n","cv2.destroyAllWindows()"],"id":"-cJvslT71m1a"},{"source":["# torch.save(model.state_dict(), \"model.pt\")"],"cell_type":"code","metadata":{"id":"w7EFRoxVXbdl"},"id":"w7EFRoxVXbdl","execution_count":null,"outputs":[]},{"source":["# model = torch.load(\"model.pt\")"],"cell_type":"code","metadata":{"id":"Ur8i7rJPXw5F"},"id":"Ur8i7rJPXw5F","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"}},"nbformat":4,"nbformat_minor":5}